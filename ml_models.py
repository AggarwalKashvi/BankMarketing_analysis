# -*- coding: utf-8 -*-
"""ML-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RAwg0T7Mp4liHHN_IZmwBz-1XZOIKico
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bank-additional-full.csv',sep=';')

df.head()

"""###Understanding the Features


Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.


```
Input variables:
# bank client data:
1 - age (numeric)
2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
5 - default: has credit in default? (categorical: 'no','yes','unknown')
6 - housing: has housing loan? (categorical: 'no','yes','unknown')
7 - loan: has personal loan? (categorical: 'no','yes','unknown')
# related with the last contact of the current campaign:
8 - contact: contact communication type (categorical: 'cellular','telephone')
9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.
# other attributes:
12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
14 - previous: number of contacts performed before this campaign and for this client (numeric)
15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
# social and economic context attributes
16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
17 - cons.price.idx: consumer price index - monthly indicator (numeric)
18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
20 - nr.employed: number of employees - quarterly indicator (numeric)

Output variable (desired target):
21 - y - has the client subscribed a term deposit? (binary: 'yes','no')
```


"""

# Basic info about the dataset
print("Dataset Overview:")
print("Shape: ", df.shape)
print("Columns: ", list(df.columns),"\n")

# Check data types
print("Data Types: ")
print(df.dtypes)

# Check for missing values

missing_values = df.isnull().sum()
print(missing_values)

"""There are no missing values in the dataset"""

# Check for unknown values
categorical_columns = ['job','marital','education','contact','default','housing','loan','poutcome']
for col in categorical_columns:
  for col in df.columns:
    unknown_count = (df[col] == 'unknown').sum()
    total_count = len(df)
    percentage = (unknown_count / total_count) * 100
    if unknown_count > 0:
      print(f"{col} : {unknown_count} unknown values ({percentage:.2f}%)")

# Special case analysis
print("Special Cases: ")
print("- pdays: 999 means 'not previously contacted'")
pdays_999 = (df['pdays'] == 999).sum()
print(f"  - pdays=999: {pdays_999} records ({(pdays_999/len(df)*100):.1f}%)")
print()

print("- duration: should be excluded for realistic predicitive models")
print(f"  - duration=0: {(df['duration'] == 0).sum()} records")
print(f"  - Mean duration: {df['duration'].mean():.1f} seconds")
print()

print("Categorical Variable Summary: ")
for col in categorical_columns:
  if col in df.columns:
    unique_vals = df[col].unique()
    print(f"  {col}: {len(unique_vals)} categories - {list(unique_vals)}")
print()

print("DATA VISUALIZATION\n")

import os

# Create folder if it doesn't exist
os.makedirs("images", exist_ok=True)

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (15, 10)

# Create subplots
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Bank Marketing Dataset - Exploratory Data Analysis', fontsize=16, fontweight='bold')

# 1. Target Variable Distribution
ax1 = axes[0, 0]
target_counts = df['y'].value_counts()
colors = ['#e74c3c', '#2ecc71']
ax1.pie(target_counts, labels=['No', 'Yes'], autopct='%1.1f%%', colors=colors, startangle=90)
ax1.set_title('Target Variable Distribution\n(Highly Imbalanced)', fontweight='bold')

# 2. Age Distribution
ax2 = axes[0, 1]
ax2.hist(df['age'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax2.axvline(df['age'].mean(), color='red', linestyle='--', label=f'Mean: {df["age"].mean():.1f}')
ax2.set_xlabel('Age')
ax2.set_ylabel('Frequency')
ax2.set_title('Age Distribution of Customers', fontweight='bold')
ax2.legend()

# 3. Job Distribution (Top 10)
ax3 = axes[0, 2]
job_counts = df['job'].value_counts().head(10)
ax3.barh(range(len(job_counts)), job_counts.values, color='coral')
ax3.set_yticks(range(len(job_counts)))
ax3.set_yticklabels(job_counts.index)
ax3.set_xlabel('Count')
ax3.set_title('Top 10 Job Categories', fontweight='bold')
ax3.invert_yaxis()

# 4. Education Level
ax4 = axes[1, 0]
education_counts = df['education'].value_counts()
ax4.bar(range(len(education_counts)), education_counts.values, color='lightgreen', edgecolor='black')
ax4.set_xticks(range(len(education_counts)))
ax4.set_xticklabels(education_counts.index, rotation=45, ha='right')
ax4.set_ylabel('Count')
ax4.set_title('Education Level Distribution', fontweight='bold')

# 5. Marital Status
ax5 = axes[1, 1]
marital_counts = df['marital'].value_counts()
wedges, texts, autotexts = ax5.pie(marital_counts, labels=marital_counts.index,
                                     autopct='%1.1f%%', startangle=90)
ax5.set_title('Marital Status Distribution', fontweight='bold')

# 6. Unknown Values Analysis
ax6 = axes[1, 2]
unknown_data = []
for col in ['job', 'marital', 'education', 'default', 'housing', 'loan']:
    unknown_count = (df[col] == 'unknown').sum()
    unknown_data.append({'Feature': col, 'Unknown': unknown_count})
unknown_df = pd.DataFrame(unknown_data)
ax6.barh(unknown_df['Feature'], unknown_df['Unknown'], color='orange')
ax6.set_xlabel('Count of Unknown Values')
ax6.set_title('Unknown Values by Feature', fontweight='bold')

plt.tight_layout()
plt.savefig('images/data_exploration.png', dpi=300, bbox_inches='tight')
plt.show()

"""###Understanding the task"""

# Step 1: Select only bank client information features
bank_features = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan']
target = 'y'

# Step 2: Create working dataset with selected features
df_bank = df[bank_features + [target]].copy()

# Encode target
y = (df_bank[target] == 'yes').astype(int)

# Prepare features
X = df_bank.drop(target, axis=1)

# Identify numeric and categorical columns
numeric_features = ['age']
categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan']

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numeric_features),
        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)
    ],
    remainder='drop'
)

# Fit and transform
X_encoded = preprocessor.fit_transform(X)

# Extract feature names correctly
numeric_feature_names = numeric_features
categorical_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)
all_features = list(numeric_feature_names) + list(categorical_feature_names)

# Convert to DataFrame
X_final = pd.DataFrame(X_encoded, columns=all_features)

print("Final encoded shape:", X_final.shape)
print("Feature names example:")
print(all_features[:10])

# Alternative approach: Manual encoding for more control

print("ALTERNATIVE APPROACH: Manual Encoding")

# Create a copy for manual encoding
df_manual = df[bank_features + [target]].copy()

# Encode target variable
y_manual = (df_manual[target] == 'yes').astype(int)

# Manual encoding of categorical variables
X_manual = df_manual.drop(target, axis=1).copy()

# Keep age as numeric
X_manual['age'] = X_manual['age']

# Encode binary categorical variables (yes/no/unknown)
binary_cats = ['default', 'housing', 'loan']
for col in binary_cats:
    # Create dummy variables
    X_manual[f'{col}_yes'] = (X_manual[col] == 'yes').astype(int)
    X_manual[f'{col}_unknown'] = (X_manual[col] == 'unknown').astype(int)
    # 'no' is the reference category (all zeros)

# Encode multi-category variables using pandas get_dummies
multi_cats = ['job', 'marital', 'education']
for col in multi_cats:
    dummies = pd.get_dummies(X_manual[col], prefix=col, drop_first=True)
    X_manual = pd.concat([X_manual, dummies], axis=1)

# Drop original categorical columns
X_manual = X_manual.drop(categorical_features, axis=1)

print(f"Manual encoding complete!")
print(f"Final shape: {X_manual.shape}")
print(f"Feature names: {list(X_manual.columns)}")
print()
print("Sample of manually encoded data:")
print(X_manual.head())

"""###Train/Test Split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split (
    X_final,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

"""Why stratification is important

Dataset is imbalanced (very few "yes" responses).
Without stratification, test set may end up with almost no positive samples, making evaluation meaningless.

###BASELINE MODEL

before using ML models, we need to create a basic reference model to judge improvement.
"""

from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import numpy as np

print("STRATEGY 1: Majority Class Baseline")

# Create majority class baseline
majority_baseline = DummyClassifier(strategy="most_frequent",random_state=42)
majority_baseline.fit(X_train, y_train)

# Make predictions
majority_predictions = majority_baseline.predict(X_test)

# Calculate metrics
majority_accuracy = accuracy_score(y_test, majority_predictions)
majority_precision = precision_score(y_test, majority_predictions, zero_division=0)
majority_recall = recall_score(y_test, majority_predictions, zero_division=0)
majority_f1 = f1_score(y_test, majority_predictions, zero_division=0)


print(f"  Strategy: Always predict the most frequent class (Class 0 - No)")
print(f"  Baseline Accuracy: {majority_accuracy:.4f} ({majority_accuracy*100:.2f}%)")
print(f"  Precision: {majority_precision:.4f}")
print(f"  Recall: {majority_recall:.4f}")
print(f"  F1-Score: {majority_f1:.4f}")

# Strategy 2: Stratified Random Baseline
print("STRATEGY 2: Stratified Random Baseline")

stratified_baseline = DummyClassifier(strategy='stratified', random_state=42)
stratified_baseline.fit(X_train, y_train)
stratified_predictions = stratified_baseline.predict(X_test)
stratified_accuracy = accuracy_score(y_test, stratified_predictions)

print(f"  Strategy: Random predictions respecting class distribution")
print(f"  Baseline Accuracy: {stratified_accuracy:.4f} ({stratified_accuracy*100:.2f}%)")

# Strategy 3: Random Uniform Baseline
print("STRATEGY 3: Uniform Random Baseline")

uniform_baseline = DummyClassifier(strategy='uniform', random_state=42)
uniform_baseline.fit(X_train, y_train)
uniform_predictions = uniform_baseline.predict(X_test)
uniform_accuracy = accuracy_score(y_test, uniform_predictions)

print(f"  Uniform Random Baseline:")
print(f"  Strategy: Completely random predictions (50/50)")
print(f"  Baseline Accuracy: {uniform_accuracy:.4f} ({uniform_accuracy*100:.2f}%)")

print("\nüìä BASELINE MODEL VISUALIZATION")
print("="*50)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle('Baseline Model Performance Analysis', fontsize=14, fontweight='bold')

# 1. Baseline Comparison
ax1 = axes[0]
baseline_models = ['Majority\nClass', 'Stratified\nRandom', 'Uniform\nRandom']
baseline_scores = [majority_accuracy, stratified_accuracy, uniform_accuracy]
colors_base = ['#e74c3c', '#f39c12', '#95a5a6']

bars = ax1.bar(baseline_models, baseline_scores, color=colors_base, edgecolor='black', alpha=0.8)
ax1.axhline(y=0.5, color='gray', linestyle='--', label='Random Guess (50%)')
ax1.set_ylabel('Accuracy', fontweight='bold')
ax1.set_title('Baseline Model Comparison', fontweight='bold')
ax1.set_ylim([0, 1])
ax1.legend()

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.1%}', ha='center', va='bottom', fontweight='bold')

# 2. Class Distribution
ax2 = axes[1]
class_data = pd.DataFrame({
    'Class': ['No (0)', 'Yes (1)'],
    'Train': [(y_train == 0).sum(), (y_train == 1).sum()],
    'Test': [(y_test == 0).sum(), (y_test == 1).sum()]
})

x = np.arange(len(class_data['Class']))
width = 0.35

bars1 = ax2.bar(x - width/2, class_data['Train'], width, label='Train', color='#3498db', alpha=0.8)
bars2 = ax2.bar(x + width/2, class_data['Test'], width, label='Test', color='#e74c3c', alpha=0.8)

ax2.set_xlabel('Class', fontweight='bold')
ax2.set_ylabel('Count', fontweight='bold')
ax2.set_title('Class Distribution (Imbalanced Data)', fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(class_data['Class'])
ax2.legend()

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                 f'{int(height):,}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig('images/baseline_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úÖ Baseline visualization saved to 'images/baseline_analysis.png'")

"""### A SIMPLE MODEL

Use Logistic Regression to build a classic model
"""

# A Simple Model - Logistic Regression

from sklearn.linear_model import LogisticRegression
import time
import numpy as np

print("LOGISTIC REGRESSION MODEL")

# Initialize the model
logistic_model = LogisticRegression(random_state=42, max_iter=1000)

# Measure training time
print("Training the Model...")
start_time = time.time()

# Fit the model
logistic_model.fit(X_train, y_train)

# Calculate training time
training_time = time.time() - start_time

print(f"Training Complete!")
print(f"Training time: {training_time:.4f} seconds")
print()

# Make predictions
train_predictions = logistic_model.predict(X_train)
test_predictions = logistic_model.predict(X_test)

# Get prediction probabilities
train_probabilities = logistic_model.predict_proba(X_train)[:, 1]  # Probability of positive class
test_probabilities = logistic_model.predict_proba(X_test)[:, 1]

print("Training Complete!")

p;# Score the Model - What is the accuracy of your model?

from sklearn.metrics import accuracy_score
import numpy as np

# Calculate accuracies from Problem 8 predictions
train_accuracy = accuracy_score(y_train, train_predictions)
test_accuracy = accuracy_score(y_test, test_predictions)

baseline_accuracy = majority_accuracy  # Majority class baseline

# Calculate and display the accuracy of our Logistic Regression model
print("Logistic Regression Model Accuracy:")
print(f"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
print(f"Test Accuracy:     {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print()

# Answer the specific question
print("ANSWER: What is the accuracy of the model?")
print(f"The Logistic Regression model achieves {test_accuracy:.4f} ({test_accuracy*100:.2f}%) accuracy on the test set.")
print()

# Context and interpretation
print("Performance Context:")
print(f"‚Ä¢ Beats baseline by: {(test_accuracy - baseline_accuracy)*100:+.2f} percentage points")
print(f"‚Ä¢ Overfitting check: {abs(train_accuracy - test_accuracy)*100:.2f}% difference between train/test")

if abs(train_accuracy - test_accuracy) < 0.05:
    print("   Low overfitting - good generalization")
else:
    print("   Potential overfitting detected")

print()
print(" Business Interpretation:")
print(f"‚Ä¢ Out of 100 customers, model correctly predicts {int(test_accuracy*100)} outcomes")
print(f"‚Ä¢ This represents a {((test_accuracy - baseline_accuracy)/baseline_accuracy)*100:+.1f}% improvement over baseline")

"""###MODEL COMPARSIONS

Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models. Using the default settings for each of the models, fit and score each. Also, be sure to compare the fit time of each of the models. Present your findings in a `DataFrame` similar to that below:
"""

# Model Comparisons - Compare multiple classification algorithms

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import pandas as pd

print("MODEL COMPARISON\n")

# Initialize models with default parameters
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
}

# Store results
results = []

print("Training and Evaluating Models:\n")

for model_name, model in models.items():
    print(f"\n Training {model_name}...")

    # Measure training time
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # Make predictions
    train_predictions = model.predict(X_train)
    test_predictions = model.predict(X_test)

    # Calculate metrics
    train_accuracy = accuracy_score(y_train, train_predictions)
    test_accuracy = accuracy_score(y_test, test_predictions)
    test_precision = precision_score(y_test, test_predictions, zero_division=0)
    test_recall = recall_score(y_test, test_predictions, zero_division=0)
    test_f1 = f1_score(y_test, test_predictions, zero_division=0)

    # Store results
    results.append({
        'Model': model_name,
        'Train Time': train_time,
        'Train Accuracy': train_accuracy,
        'Test Accuracy': test_accuracy,
        'Precision': test_precision,
        'Recall': test_recall,
        'F1-Score': test_f1
    })

    print(f" {model_name} completed in {train_time:.4f} seconds")
    print(f"   Test Accuracy: {test_accuracy:.4f}\n")

print(" MODEL COMPARISON RESULTS\n")

# Create DataFrame for comparison
results_df = pd.DataFrame(results)

# Display the requested table format
print("\n Performance Summary Table:")
print(f"{'Model':<25} {'Train Time':<12} {'Train Accuracy':<15} {'Test Accuracy':<15}")

for _, row in results_df.iterrows():
    print(f"{row['Model']:<25} {row['Train Time']:<12.4f} {row['Train Accuracy']:<15.4f} {row['Test Accuracy']:<15.4f}")

print("-" * 70)

# Extended metrics table
print("\n Extended Performance Metrics:\n")
print(f"{'Model':<25} {'Test Acc':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Train Time':<12}\n")

for _, row in results_df.iterrows():
    print(f"{row['Model']:<25} {row['Test Accuracy']:<10.4f} {row['Precision']:<10.4f} "
          f"{row['Recall']:<10.4f} {row['F1-Score']:<10.4f} {row['Train Time']:<12.4f}")

print("\nüìä MODEL COMPARISON VISUALIZATIONS")
print("="*60)

fig = plt.figure(figsize=(20, 18))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

fig.suptitle('Classification Models Comparison - Comprehensive Analysis',
             fontsize=16, fontweight='bold')

# 1. Test Accuracy Comparison
ax1 = fig.add_subplot(gs[0, 0])
models_list = results_df['Model'].tolist()
test_accs = results_df['Test Accuracy'].tolist()
colors_models = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']

bars = ax1.barh(models_list, test_accs, color=colors_models, edgecolor='black', alpha=0.8)
ax1.axvline(x=baseline_accuracy, color='red', linestyle='--', linewidth=2, label=f'Baseline: {baseline_accuracy:.3f}')
ax1.set_xlabel('Test Accuracy', fontweight='bold')
ax1.set_title('Test Accuracy Comparison', fontweight='bold')
ax1.set_xlim([0.85, 0.92])
ax1.legend()

for i, (bar, val) in enumerate(zip(bars, test_accs)):
    ax1.text(val, bar.get_y() + bar.get_height()/2, f'{val:.4f}',
             va='center', ha='left', fontweight='bold', fontsize=9)

# 2. F1-Score Comparison
ax2 = fig.add_subplot(gs[0, 1])
f1_scores = results_df['F1-Score'].tolist()
bars = ax2.bar(range(len(models_list)), f1_scores, color=colors_models,
               edgecolor='black', alpha=0.8)
ax2.set_xticks(range(len(models_list)))
ax2.set_xticklabels([m.replace(' ', '\n') for m in models_list], fontsize=9)
ax2.set_ylabel('F1-Score', fontweight='bold')
ax2.set_title('F1-Score Comparison\n(Balanced Metric)', fontweight='bold')

for bar, val in zip(bars, f1_scores):
    ax2.text(bar.get_x() + bar.get_width()/2, val, f'{val:.3f}',
             ha='center', va='bottom', fontweight='bold', fontsize=9)

# 3. Training Time Comparison (Log Scale)
ax3 = fig.add_subplot(gs[0, 2])
train_times = results_df['Train Time'].tolist()
bars = ax3.bar(range(len(models_list)), train_times, color=colors_models,
               edgecolor='black', alpha=0.8)
ax3.set_yscale('log')
ax3.set_xticks(range(len(models_list)))
ax3.set_xticklabels([m.replace(' ', '\n') for m in models_list], fontsize=9)
ax3.set_ylabel('Training Time (seconds, log scale)', fontweight='bold')
ax3.set_title('Training Time Comparison', fontweight='bold')

for bar, val in zip(bars, train_times):
    ax3.text(bar.get_x() + bar.get_width()/2, val, f'{val:.3f}s',
             ha='center', va='bottom', fontsize=8)

# 4. Precision vs Recall
ax4 = fig.add_subplot(gs[1, 0])
precisions = results_df['Precision'].tolist()
recalls = results_df['Recall'].tolist()

for i, (model, prec, rec, color) in enumerate(zip(models_list, precisions, recalls, colors_models)):
    ax4.scatter(rec, prec, s=300, color=color, alpha=0.6, edgecolors='black', linewidth=2)
    ax4.annotate(model.replace(' ', '\n'), (rec, prec),
                ha='center', va='center', fontsize=8, fontweight='bold')

ax4.set_xlabel('Recall', fontweight='bold')
ax4.set_ylabel('Precision', fontweight='bold')
ax4.set_title('Precision vs Recall Trade-off', fontweight='bold')
ax4.set_xlim([-0.05, 1.05])
ax4.set_ylim([-0.05, 1.05])
ax4.grid(True, alpha=0.3)
ax4.axline((0, 0), (1, 1), color='gray', linestyle='--', alpha=0.5)

# 5. Multi-Metric Radar Chart
ax5 = fig.add_subplot(gs[1, 1], projection='polar')
categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
angles += angles[:1]

for i, (model, color) in enumerate(zip(models_list, colors_models)):
    values = [
        results_df.iloc[i]['Test Accuracy'],
        results_df.iloc[i]['Precision'],
        results_df.iloc[i]['Recall'],
        results_df.iloc[i]['F1-Score']
    ]
    values += values[:1]
    ax5.plot(angles, values, 'o-', linewidth=2, label=model, color=color)
    ax5.fill(angles, values, alpha=0.15, color=color)

ax5.set_xticks(angles[:-1])
ax5.set_xticklabels(categories, fontsize=10)
ax5.set_ylim(0, 1)
ax5.set_title('Multi-Metric Performance\n(Radar Chart)', fontweight='bold', pad=20)
ax5.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
ax5.grid(True)

# 6. Performance Metrics Heatmap
ax6 = fig.add_subplot(gs[1, 2])
metrics_data = results_df[['Test Accuracy', 'Precision', 'Recall', 'F1-Score']].values
sns.heatmap(metrics_data, annot=True, fmt='.3f', cmap='RdYlGn',
            xticklabels=['Accuracy', 'Precision', 'Recall', 'F1'],
            yticklabels=[m.split()[0] for m in models_list],
            cbar_kws={'label': 'Score'}, ax=ax6, vmin=0, vmax=1)
ax6.set_title('Performance Metrics Heatmap', fontweight='bold')

# 7. Train vs Test Accuracy
ax7 = fig.add_subplot(gs[2, 0])
train_accs = results_df['Train Accuracy'].tolist()
x = np.arange(len(models_list))
width = 0.35

bars1 = ax7.bar(x - width/2, train_accs, width, label='Train',
                color='#3498db', alpha=0.8, edgecolor='black')
bars2 = ax7.bar(x + width/2, test_accs, width, label='Test',
                color='#e74c3c', alpha=0.8, edgecolor='black')

ax7.set_ylabel('Accuracy', fontweight='bold')
ax7.set_title('Train vs Test Accuracy\n(Overfitting Check)', fontweight='bold')
ax7.set_xticks(x)
ax7.set_xticklabels([m.replace(' ', '\n') for m in models_list], fontsize=9)
ax7.legend()
ax7.set_ylim([0.85, 1.0])

# 8. Performance Summary Table
ax8 = fig.add_subplot(gs[2, 1:])
ax8.axis('tight')
ax8.axis('off')

table_data = []
for _, row in results_df.iterrows():
    table_data.append([
        row['Model'],
        f"{row['Test Accuracy']:.4f}",
        f"{row['Precision']:.4f}",
        f"{row['Recall']:.4f}",
        f"{row['F1-Score']:.4f}",
        f"{row['Train Time']:.4f}s"
    ])

table = ax8.table(cellText=table_data,
                  colLabels=['Model', 'Test Acc', 'Precision', 'Recall', 'F1-Score', 'Train Time'],
                  cellLoc='center',
                  loc='center',
                  colWidths=[0.25, 0.15, 0.15, 0.15, 0.15, 0.15])

table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 2)

# Color header
for i in range(6):
    table[(0, i)].set_facecolor('#3498db')
    table[(0, i)].set_text_props(weight='bold', color='white')

# Color rows
for i in range(1, len(table_data) + 1):
    for j in range(6):
        if i % 2 == 0:
            table[(i, j)].set_facecolor('#ecf0f1')

ax8.set_title('Comprehensive Performance Summary', fontweight='bold', pad=20, fontsize=12)

plt.savefig('images/model_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úÖ Model comparison visualizations saved to 'images/model_comparison.png'")

"""### IMPROVING THE MODEL

Now that we have some basic models on the board, we want to try to improve these. Below, we list a few things to explore in this pursuit.

Hyperparameter tuning and grid search. All of our models have additional hyperparameters to tune and explore. For example the number of neighbors in KNN or the maximum depth of a Decision Tree.
Adjust your performance metric
"""

# Improving the Model - Hyperparameter Tuning & Performance Optimization

from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
import pandas as pd
import warnings
import time

warnings.filterwarnings('ignore')

print("MODEL IMPROVEMENT - Problem 11")
print("="*60)

print("Improvement strategies:")
print("1. Hyperparameter tuning")
print("2. Alternative evaluation metrics")
print("3. Cross-validation")
print("4. Feature scaling where required")
print()

# Hyperparameter grids
param_grids = {
    'Logistic Regression': {
        'C': [0.1, 1, 10],
        'penalty': ['l2'],
        'solver': ['lbfgs'],
        'max_iter': [1000]
    },
    'K-Nearest Neighbors': {
        'n_neighbors': [3, 5, 7, 11],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean']
    },
    'Decision Tree': {
        'max_depth': [3, 5, 10, None],
        'min_samples_split': [2, 10],
        'criterion': ['gini']
    }
}

# Feature scaling (only required for distance-based models)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Features scaled using StandardScaler where required")
print()

# Define models
models_to_tune = {
    'Logistic Regression': (LogisticRegression(random_state=42), X_train, X_test),
    'K-Nearest Neighbors': (KNeighborsClassifier(), X_train_scaled, X_test_scaled),
    'Decision Tree': (DecisionTreeClassifier(random_state=42), X_train, X_test)
}

improved_results = []
best_models = {}

print("Running Grid Search tuning...")
print()

# Grid Search Loop
for model_name, (model, X_tr, X_te) in models_to_tune.items():
    print(f"Tuning {model_name}...")

    start_time = time.time()

    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grids[model_name],
        cv=3,
        scoring='f1',
        n_jobs=-1,
        verbose=0
    )

    grid_search.fit(X_tr, y_train)

    best_model = grid_search.best_estimator_
    best_models[model_name] = best_model

    train_pred = best_model.predict(X_tr)
    test_pred = best_model.predict(X_te)
    proba = best_model.predict_proba(X_te)[:, 1] if hasattr(best_model, 'predict_proba') else None

    train_acc = accuracy_score(y_train, train_pred)
    test_acc = accuracy_score(y_test, test_pred)
    precision = precision_score(y_test, test_pred, zero_division=0)
    recall = recall_score(y_test, test_pred, zero_division=0)
    f1 = f1_score(y_test, test_pred, zero_division=0)
    roc_auc = roc_auc_score(y_test, proba) if proba is not None else None

    cv_scores = cross_val_score(best_model, X_tr, y_train, cv=3, scoring='f1')

    elapsed_time = time.time() - start_time

    improved_results.append({
        'Model': model_name,
        'Best Parameters': grid_search.best_params_,
        'CV Mean F1': round(cv_scores.mean(), 4),
        'Train Accuracy': round(train_acc, 4),
        'Test Accuracy': round(test_acc, 4),
        'Precision': round(precision, 4),
        'Recall': round(recall, 4),
        'F1 Score': round(f1, 4),
        'ROC AUC': round(roc_auc, 4) if roc_auc is not None else None,
        'Training Time (s)': round(elapsed_time, 2)
    })

improved_df = pd.DataFrame(improved_results)

print("\nFinal Model Comparison:")
print(improved_df.to_string(index=False))
print("\nBest Hyperparameters:")
for model_info in improved_results:
    print(f"{model_info['Model']}: {model_info['Best Parameters']}")

improved_df.rename(columns={
    'F1 Score': 'F1_Score',
    'Test Accuracy': 'Test_Accuracy',
    'ROC AUC': 'ROC_AUC',
    'Training Time (s)': 'Tune_Time',
    'CV Mean F1': 'CV_F1_Mean',
    'CV F1 Std': 'CV_F1_Std'
}, inplace=True)

# Strategy 5: Performance Comparison and Visualization
print("="*60)
print("STRATEGY 5: PERFORMANCE ANALYSIS & VISUALIZATION")
print("="*60)

# Build comparison table between original and improved models
comparison_data = []

for i, (_, original_row) in enumerate(results_df.iterrows()):
    improved_row = improved_df.iloc[i]

    f1_improvement = improved_row['F1_Score'] - original_row['F1-Score']
    acc_improvement = improved_row['Test_Accuracy'] - original_row['Test Accuracy']

    comparison_data.append({
        'Model': original_row['Model'],
        'Original_Accuracy': original_row['Test Accuracy'],
        'Improved_Accuracy': improved_row['Test_Accuracy'],
        'Accuracy_Gain': acc_improvement,
        'Original_F1': original_row['F1-Score'],
        'Improved_F1': improved_row['F1_Score'],
        'F1_Gain': f1_improvement
    })

comparison_df = pd.DataFrame(comparison_data)

print(f"{'Model':<25} {'Acc Gain':<10} {'F1 Gain':<10} {'Status':<15}")
print("-" * 60)

for _, row in comparison_df.iterrows():
    if row['F1_Gain'] > 0:
        status = "IMPROVED"
    elif row['F1_Gain'] == 0:
        status = "NO CHANGE"
    else:
        status = "DECREASED"

    print(f"{row['Model']:<25} {row['Accuracy_Gain']:+8.4f} {row['F1_Gain']:+8.4f} {status:<15}")

print()

# Determine best models based on final metrics
best_f1_model = improved_df.loc[improved_df['F1_Score'].idxmax()]
best_auc_model = improved_df.loc[improved_df['ROC_AUC'].idxmax()]

print("BEST PERFORMING MODELS:")
print(f"Best F1-Score: {best_f1_model['Model']} ({best_f1_model['F1_Score']:.4f})")
print(f"Best ROC-AUC: {best_auc_model['Model']} ({best_auc_model['ROC_AUC']:.4f})")
print()

# Detailed comparison
print("DETAILED COMPARISON TABLE:")
print("-" * 120)
print(f"{'Model':<25} {'Original F1':<12} {'Improved F1':<12} {'Original Acc':<12} {'Improved Acc':<12} {'Status'}")
print("-" * 120)

for _, row in comparison_df.iterrows():
    if row['F1_Gain'] > 0.01:
        status = "SIGNIFICANT IMPROVEMENT"
    elif row['F1_Gain'] > 0:
        status = "MINOR IMPROVEMENT"
    else:
        status = "WORSE"

    print(f"{row['Model']:<25} {row['Original_F1']:<12.4f} {row['Improved_F1']:<12.4f} "
          f"{row['Original_Accuracy']:<12.4f} {row['Improved_Accuracy']:<12.4f} {status}")

print("-" * 120)


# Strategy 6: Business Impact Analysis
print("\nSTRATEGY 6: BUSINESS IMPACT ANALYSIS")
print("="*50)

total_customers = len(y_test)
actual_positives = (y_test == 1).sum()
baseline_accuracy_stored = baseline_accuracy  # from earlier

print("BUSINESS METRICS:")
print(f"Total test customers: {total_customers}")
print(f"Actual subscribers: {actual_positives}")
print(f"Baseline accuracy: {baseline_accuracy_stored:.4f}")
print()

# Business impact estimation
print("MARKETING EFFICIENCY ANALYSIS:")
print("-" * 80)
print(f"{'Model':<25} {'Contacts':<10} {'Conversions':<12} {'Efficiency':<12} {'ROI':<10}")
print("-" * 80)

for _, row in improved_df.iterrows():
    precision = row['Precision']
    recall = row['Recall']

    if recall > 0:
        predicted_positives = actual_positives / recall
        true_positives = predicted_positives * precision if precision > 0 else 0

        marketing_efficiency = (true_positives / predicted_positives * 100)

        cost_per_call = 5
        revenue_per_conversion = 100
        total_cost = predicted_positives * cost_per_call
        total_revenue = true_positives * revenue_per_conversion
        roi = ((total_revenue - total_cost) / total_cost * 100)

        print(f"{row['Model']:<25} {predicted_positives:<10.0f} {true_positives:<12.0f} "
              f"{marketing_efficiency:<12.1f}% {roi:<10.1f}%")
    else:
        print(f"{row['Model']:<25} {'N/A':<10} {'N/A':<12} {'N/A':<12} {'N/A':<10}")

print("-" * 80)

# Rankings
print("\nMODEL RANKINGS:")
print("="*30)

print("F1-Score Ranking:")
for i, (_, row) in enumerate(improved_df.sort_values('F1_Score', ascending=False).iterrows(), 1):
    print(f"{i}. {row['Model']}: {row['F1_Score']:.4f}")

print("\nROC-AUC Ranking:")
for i, (_, row) in enumerate(improved_df.sort_values('ROC_AUC', ascending=False).iterrows(), 1):
    print(f"{i}. {row['Model']}: {row['ROC_AUC']:.4f}")

print("\nTraining Time Ranking (Fastest to Slowest):")
for i, (_, row) in enumerate(improved_df.sort_values('Tune_Time', ascending=True).iterrows(), 1):
    print(f"{i}. {row['Model']}: {row['Tune_Time']:.2f} seconds")


# Final recommendations
print("\nFINAL RECOMMENDATIONS:")
print("="*40)
print("1. Model Selection:")
print(f"   Best Overall: {best_f1_model['Model']} (F1: {best_f1_model['F1_Score']:.4f})")
print(f"   Best for Ranking Customers: {best_auc_model['Model']} (AUC: {best_auc_model['ROC_AUC']:.4f})")
print(f"   Fastest Training: {improved_df.sort_values('Tune_Time', ascending=True).iloc[0]['Model']}")

print("\n2. Deployment Strategy:")
if best_f1_model['F1_Score'] > baseline_accuracy_stored:
    print("   Ready for deployment")
else:
    print("   Further improvement recommended")

print("\n3. Next Steps:")
print("   - Monitor performance on live campaign data")
print("   - Consider additional feature engineering or ensemble methods")


# Summary
print("\nSUMMARY STATISTICS:")
print(f"Average accuracy improvement: {comparison_df['Accuracy_Gain'].mean():+.4f}")
print(f"Average F1 improvement: {comparison_df['F1_Gain'].mean():+.4f}")
print(f"Best F1 achieved: {improved_df['F1_Score'].max():.4f}")
print(f"Best ROC-AUC achieved: {improved_df['ROC_AUC'].max():.4f}")
print(f"Total tuning time: {improved_df['Tune_Time'].sum():.1f} seconds")

print("\nBest model ready for deployment:")
print(best_f1_model['Model'])

# Create ranking tables based on metrics
f1_ranking = improved_df.sort_values('F1_Score', ascending=False)
auc_ranking = improved_df.sort_values('ROC_AUC', ascending=False)
time_ranking = improved_df.sort_values('Tune_Time', ascending=True)

print("\nüìä IMPROVED MODELS VISUALIZATIONS")
print("="*60)

fig = plt.figure(figsize=(20, 22))
gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3)

fig.suptitle('Hyperparameter Tuning Results - Before vs After Comparison',
             fontsize=16, fontweight='bold')

# 1. F1-Score Improvement
ax1 = fig.add_subplot(gs[0, :])
x_pos = np.arange(len(comparison_df))
width = 0.35

bars1 = ax1.bar(x_pos - width/2, comparison_df['Original_F1'], width,
                label='Before Tuning', color='#95a5a6', alpha=0.8, edgecolor='black')
bars2 = ax1.bar(x_pos + width/2, comparison_df['Improved_F1'], width,
                label='After Tuning', color='#2ecc71', alpha=0.8, edgecolor='black')

ax1.set_ylabel('F1-Score', fontweight='bold', fontsize=12)
ax1.set_title('F1-Score Improvement After Hyperparameter Tuning', fontweight='bold', fontsize=13)
ax1.set_xticks(x_pos)
ax1.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')
ax1.legend(fontsize=11)
ax1.grid(axis='y', alpha=0.3)

# Add improvement arrows
for i, (orig, impr) in enumerate(zip(comparison_df['Original_F1'], comparison_df['Improved_F1'])):
    if impr > orig:
        ax1.annotate('', xy=(i, impr), xytext=(i, orig),
                    arrowprops=dict(arrowstyle='->', color='green', lw=2))
        ax1.text(i, impr + 0.01, f'+{(impr-orig):.3f}',
                ha='center', fontsize=9, color='green', fontweight='bold')

# 2. Accuracy Improvement
ax2 = fig.add_subplot(gs[1, 0])
bars = ax2.barh(comparison_df['Model'], comparison_df['Accuracy_Gain'],
                color=['#2ecc71' if x > 0 else '#e74c3c' for x in comparison_df['Accuracy_Gain']],
                edgecolor='black', alpha=0.8)
ax2.set_xlabel('Accuracy Gain', fontweight='bold')
ax2.set_title('Accuracy Improvement', fontweight='bold')
ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
ax2.grid(axis='x', alpha=0.3)

for bar, val in zip(bars, comparison_df['Accuracy_Gain']):
    ax2.text(val, bar.get_y() + bar.get_height()/2, f'{val:+.4f}',
             va='center', ha='left' if val > 0 else 'right', fontweight='bold', fontsize=9)

# 3. ROC-AUC Scores
ax3 = fig.add_subplot(gs[1, 1])
bars = ax3.bar(range(len(improved_df)), improved_df['ROC_AUC'],
               color=colors_models, edgecolor='black', alpha=0.8)
ax3.set_xticks(range(len(improved_df)))
ax3.set_xticklabels([m.replace(' ', '\n') for m in improved_df['Model']], fontsize=9)
ax3.set_ylabel('ROC-AUC Score', fontweight='bold')
ax3.set_title('ROC-AUC Performance\n(Customer Ranking Ability)', fontweight='bold')
ax3.set_ylim([0, 1])
ax3.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')
ax3.legend()

for bar, val in zip(bars, improved_df['ROC_AUC']):
    ax3.text(bar.get_x() + bar.get_width()/2, val, f'{val:.3f}',
             ha='center', va='bottom', fontweight='bold', fontsize=9)

# 4. Cross-Validation Stability
ax4 = fig.add_subplot(gs[1, 2])
cv_means = improved_df['CV_F1_Mean'].tolist()

bars = ax4.bar(range(len(improved_df)), cv_means,
               color=colors, edgecolor='black', alpha=0.8)

ax4.set_xticks(range(len(improved_df)))
ax4.set_xticklabels([m.replace(' ', '\n') for m in improved_df['Model']], fontsize=9)
ax4.set_ylabel('CV F1-Score', fontweight='bold')
ax4.set_title('Cross-Validation F1-Score\n(with std dev)', fontweight='bold')

# 5. Precision vs Recall (Improved)
ax5 = fig.add_subplot(gs[2, 0])
for i, (model, color) in enumerate(zip(improved_df['Model'], colors_models)):
    prec = improved_df.iloc[i]['Precision']
    rec = improved_df.iloc[i]['Recall']
    ax5.scatter(rec, prec, s=400, color=color, alpha=0.6,
               edgecolors='black', linewidth=2, label=model)
    ax5.annotate(f"{model.split()[0]}\n({prec:.2f}, {rec:.2f})",
                (rec, prec), ha='center', va='center', fontsize=8, fontweight='bold')

ax5.set_xlabel('Recall', fontweight='bold')
ax5.set_ylabel('Precision', fontweight='bold')
ax5.set_title('Precision-Recall Trade-off\n(After Tuning)', fontweight='bold')
ax5.set_xlim([-0.05, 1.05])
ax5.set_ylim([-0.05, 1.05])
ax5.grid(True, alpha=0.3)
ax5.axline((0, 0), (1, 1), color='gray', linestyle='--', alpha=0.5)

# 6. Training Time Comparison
ax6 = fig.add_subplot(gs[2, 1])
tune_times = improved_df['Tune_Time'].tolist()
bars = ax6.barh(improved_df['Model'], tune_times, color=colors_models,
                edgecolor='black', alpha=0.8)
ax6.set_xlabel('Tuning Time (seconds)', fontweight='bold')
ax6.set_title('Hyperparameter Tuning Time', fontweight='bold')
ax6.set_xscale('log')

for bar, val in zip(bars, tune_times):
    ax6.text(val, bar.get_y() + bar.get_height()/2, f'{val:.1f}s',
             va='center', ha='left', fontweight='bold', fontsize=9)

# 7. Business Impact - Marketing Efficiency
ax7 = fig.add_subplot(gs[2, 2])
# Calculate marketing efficiency for each model
efficiencies = []
for _, row in improved_df.iterrows():
    if row['Recall'] > 0 and row['Precision'] > 0:
        efficiency = row['Precision'] * 100
        efficiencies.append(efficiency)
    else:
        efficiencies.append(0)

bars = ax7.bar(range(len(improved_df)), efficiencies, color=colors_models,
               edgecolor='black', alpha=0.8)
ax7.set_xticks(range(len(improved_df)))
ax7.set_xticklabels([m.replace(' ', '\n') for m in improved_df['Model']], fontsize=9)
ax7.set_ylabel('Marketing Efficiency (%)', fontweight='bold')
ax7.set_title('Marketing Conversion Efficiency\n(Precision %)', fontweight='bold')

for bar, val in zip(bars, efficiencies):
    ax7.text(bar.get_x() + bar.get_width()/2, val, f'{val:.1f}%',
             ha='center', va='bottom', fontweight='bold', fontsize=9)

# 8. Model Rankings Summary
ax8 = fig.add_subplot(gs[3, :])
ax8.axis('tight')
ax8.axis('off')

ranking_data = [
    ['Metric', 'ü•á 1st Place', 'ü•à 2nd Place', 'ü•â 3rd Place'],
    ['F1-Score',
     f"{f1_ranking.iloc[0]['Model']} ({f1_ranking.iloc[0]['F1_Score']:.3f})",
     f"{f1_ranking.iloc[1]['Model']} ({f1_ranking.iloc[1]['F1_Score']:.3f})",
     f"{f1_ranking.iloc[2]['Model']} ({f1_ranking.iloc[2]['F1_Score']:.3f})"],
    ['ROC-AUC',
     f"{auc_ranking.iloc[0]['Model']} ({auc_ranking.iloc[0]['ROC_AUC']:.3f})",
     f"{auc_ranking.iloc[1]['Model']} ({auc_ranking.iloc[1]['ROC_AUC']:.3f})",
     f"{auc_ranking.iloc[2]['Model']} ({auc_ranking.iloc[2]['ROC_AUC']:.3f})"],
    ['Speed',
     f"{time_ranking.iloc[0]['Model']} ({time_ranking.iloc[0]['Tune_Time']:.1f}s)",
     f"{time_ranking.iloc[1]['Model']} ({time_ranking.iloc[1]['Tune_Time']:.1f}s)",
     f"{time_ranking.iloc[2]['Model']} ({time_ranking.iloc[2]['Tune_Time']:.1f}s)"]
]

table = ax8.table(cellText=ranking_data,
                  cellLoc='center',
                  loc='center',
                  colWidths=[0.15, 0.28, 0.28, 0.28])

table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 2.5)

# Style header
for i in range(4):
    table[(0, i)].set_facecolor('#3498db')
    table[(0, i)].set_text_props(weight='bold', color='white', fontsize=11)

# Style rows
medal_colors = ['#FFD700', '#C0C0C0', '#CD7F32']  # Gold, Silver, Bronze
for i in range(1, 4):
    table[(i, 0)].set_facecolor('#ecf0f1')
    table[(i, 0)].set_text_props(weight='bold')
    for j in range(1, 4):
        table[(i, j)].set_facecolor(medal_colors[j-1])
        table[(i, j)].set_text_props(weight='bold')

ax8.set_title('üèÜ Model Rankings by Performance Metric üèÜ',
              fontweight='bold', pad=20, fontsize=14)

plt.savefig('images/improved_models.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úÖ Improved models visualizations saved to 'images/improved_models.png'")

print(improved_df.columns.tolist())

